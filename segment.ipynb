{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "from pycocotools import mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "import detectron2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resize_ratio(file_path):\n",
    "    #read image to resize\n",
    "    image = cv2.imread(file_path)\n",
    "    #resize to 1000*1000\n",
    "    resized_image = cv2.resize(image, (480, 480))  \n",
    "    #get resize ratio      \n",
    "    resize_ratio = resized_image.shape[0] / image.shape[0]\n",
    "    #write to file\n",
    "    cv2.imwrite(file_path, resized_image)\n",
    "    return resize_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dicts(directory, classes):\n",
    "    dataset_dicts = []\n",
    "    for i, filename in enumerate([file for file in os.listdir(directory) if file.endswith('.png')]):\n",
    "        mask_file = os.path.join(directory, filename)\n",
    "\n",
    "        #要處理每場圖片的大小 要resize\n",
    "        resize_ratio = get_resize_ratio(mask_file)\n",
    "\n",
    "        with open(mask_file.replace(\".png\", \".json\")) as f:\n",
    "            img_anns = json.load(f)\n",
    "        # 載入圖像\n",
    "        image = cv2.imread(mask_file)\n",
    "\n",
    "        lower_black = np.array([0,0,0])\n",
    "        upper_black = np.array([0,0,0])\n",
    "\n",
    "        # 檢測圖像中黑色位置\n",
    "        black_mask = cv2.inRange(image, lower_black, upper_black)\n",
    "\n",
    "        # 將黑色位置設置為零，以達到檢測除了黑色以外的所有位置的目的\n",
    "        mask = cv2.bitwise_not(black_mask)\n",
    "\n",
    "\n",
    "        # 尋找輪廓\n",
    "        contours, hierarchy  = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "                \n",
    "        record = {}\n",
    "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"height\"] = 480\n",
    "        record[\"width\"] = 480\n",
    "        record[\"image_id\"] = i\n",
    "        annos = img_anns[\"shapes\"]\n",
    "        \n",
    "\n",
    "        for contour in contours:\n",
    "            # 繪製輪廓\n",
    "            cv2.drawContours(image, contour, -1, (0,255,0), 3)\n",
    "\n",
    "        objs = []\n",
    "        for contour in contours:\n",
    "            # 遍歷輪廓中的所有點\n",
    "            px = []\n",
    "            py = []\n",
    "            points = []\n",
    "            for point in contour:\n",
    "                # 印出座標\n",
    "                px.append((point[0][0]))\n",
    "                py.append((point[0][1]))\n",
    "                points.append((point[0][0], point[0][1]))\n",
    "                poly = [point for x in points for point in x]\n",
    "                #segmentation_mask = np.asarray(poly, dtype=np.uint8)\n",
    "                #rle = segmentation_mask.encode()\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": classes.index(annos[0]['label']),\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['powder_uncover', 'powder_uneven', 'scratch']\n",
    "\n",
    "data_path = '/Users/david/Desktop/DIP_final/segmentation/data/'\n",
    "\n",
    "\n",
    "for d in [\"train\", \"test\"]:\n",
    "    DatasetCatalog.register(\n",
    "        \"category_\" + d, \n",
    "        lambda d=d: get_data_dicts(data_path+d, classes)\n",
    "    )\n",
    "    MetadataCatalog.get(\"category_\" + d).set(thing_classes=classes)\n",
    "\n",
    "microcontroller_metadata = MetadataCatalog.get(\"category_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2 import model_zoo\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"category_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.MODEL.DEVICE = 'cpu'\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 3000\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
    "cfg.OUTPUT_DIR = \"/Users/david/Desktop/DIP_final/segmentation/outputModels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/09 19:36:01 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[01/09 19:36:17 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 300 images left.\n",
      "\u001b[32m[01/09 19:36:17 d2.data.build]: \u001b[0mDistribution of instances among all 3 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:----------:|:-------------|\n",
      "| powder_unco.. | 187          | powder_uneven | 129          |  scratch   | 220          |\n",
      "|               |              |               |              |            |              |\n",
      "|     total     | 536          |               |              |            |              |\u001b[0m\n",
      "\u001b[32m[01/09 19:36:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[01/09 19:36:17 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[01/09 19:36:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[01/09 19:36:17 d2.data.common]: \u001b[0mSerializing 300 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/09 19:36:18 d2.data.common]: \u001b[0mSerialized dataset takes 11.32 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/09 19:36:18 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[01/09 19:36:21 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (12, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (12,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (3, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.DATASETS.TEST = (\"category_test\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<detectron2.engine.defaults.DefaultPredictor at 0x28b877760>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset_dicts = get_data_dicts(data_path+'test', classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17ffb33d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 讀取圖像\n",
    "image = cv2.imread(\"/Users/david/Desktop/DIP_final/segmentation/data/train_mask/converted__ 0174.png\")\n",
    "outputs = predictor(image)\n",
    "v = Visualizer(image[:, :, ::-1],\n",
    "                   metadata=microcontroller_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.IMAGE_BW # removes the colors of unsegmented pixels\n",
    ")\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    #v = v.get_output()\n",
    "image = cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #cv2.imwrite(out_path, image)\n",
    "plt.figure(figsize = (14, 10))\n",
    "plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save instance image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor d in test_dataset_dicts:    \\n    img = cv2.imread(d[\"file_name\"])\\n    outputs = predictor(img)\\n    v = Visualizer(img[:, :, ::-1],\\n                   metadata=microcontroller_metadata, \\n                   scale=1, \\n                   instance_mode=ColorMode.IMAGE_BW # removes the colors of unsegmented pixels\\n    )\\n    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\\n    #v = v.get_output()\\n    image = cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\\n    #image[image != 0] = 255\\n    filename = d[\"file_name\"]\\n    start_index = filename.find(\"test/\")\\n    substring = filename[start_index+5:]\\n    out_path = os.path.join(data_path+\\'out\\', substring)\\n    #cv2.imwrite(out_path, image)\\n    plt.figure(figsize = (14, 10))\\n    plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for d in test_dataset_dicts:    \n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "    v = Visualizer(img[:, :, ::-1],\n",
    "                   metadata=microcontroller_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.IMAGE_BW # removes the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    #v = v.get_output()\n",
    "    image = cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n",
    "    #image[image != 0] = 255\n",
    "    filename = d[\"file_name\"]\n",
    "    start_index = filename.find(\"test/\")\n",
    "    substring = filename[start_index+5:]\n",
    "    out_path = os.path.join(data_path+'out', substring)\n",
    "    #cv2.imwrite(out_path, image)\n",
    "    plt.figure(figsize = (14, 10))\n",
    "    plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m test_dataset_dicts:    \n\u001b[1;32m      3\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(d[\u001b[39m\"\u001b[39m\u001b[39mfile_name\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m     outputs \u001b[39m=\u001b[39m predictor(img) \n\u001b[1;32m      5\u001b[0m     v \u001b[39m=\u001b[39m Visualizer(img[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m      6\u001b[0m                    metadata\u001b[39m=\u001b[39mmicrocontroller_metadata, \n\u001b[1;32m      7\u001b[0m                    scale\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m      8\u001b[0m                    instance_mode\u001b[39m=\u001b[39mColorMode\u001b[39m.\u001b[39mIMAGE_BW \u001b[39m# removes the colors of unsegmented pixels\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mdraw_instance_predictions(outputs[\u001b[39m\"\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/engine/defaults.py:317\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    314\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(image\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    316\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: image, \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m: height, \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m: width}\n\u001b[0;32m--> 317\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel([inputs])[\u001b[39m0\u001b[39m]\n\u001b[1;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 150\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(batched_inputs)\n\u001b[1;32m    152\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batched_inputs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py:208\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m detected_instances \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_generator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         proposals, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproposal_generator(images, features, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mproposals\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batched_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py:454\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    451\u001b[0m features \u001b[39m=\u001b[39m [features[f] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features]\n\u001b[1;32m    452\u001b[0m anchors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_generator(features)\n\u001b[0;32m--> 454\u001b[0m pred_objectness_logits, pred_anchor_deltas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn_head(features)\n\u001b[1;32m    455\u001b[0m \u001b[39m# Transpose the Hi*Wi*A dimension to the middle:\u001b[39;00m\n\u001b[1;32m    456\u001b[0m pred_objectness_logits \u001b[39m=\u001b[39m [\n\u001b[1;32m    457\u001b[0m     \u001b[39m# (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     score\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\n\u001b[1;32m    459\u001b[0m     \u001b[39mfor\u001b[39;00m score \u001b[39min\u001b[39;00m pred_objectness_logits\n\u001b[1;32m    460\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py:174\u001b[0m, in \u001b[0;36mStandardRPNHead.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    172\u001b[0m pred_anchor_deltas \u001b[39m=\u001b[39m []\n\u001b[1;32m    173\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m features:\n\u001b[0;32m--> 174\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m    175\u001b[0m     pred_objectness_logits\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjectness_logits(t))\n\u001b[1;32m    176\u001b[0m     pred_anchor_deltas\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_deltas(t))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/layers/wrappers.py:113\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    108\u001b[0m             \u001b[39m# https://github.com/pytorch/pytorch/issues/12013\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    110\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSyncBatchNorm\n\u001b[1;32m    111\u001b[0m             ), \u001b[39m\"\u001b[39m\u001b[39mSyncBatchNorm does not support empty inputs!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 113\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m    114\u001b[0m     x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "for d in test_dataset_dicts:    \n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img) \n",
    "    v = Visualizer(img[:, :, ::-1],\n",
    "                   metadata=microcontroller_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.IMAGE_BW # removes the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    mask_array = outputs['instances'].to(\"cpu\").pred_masks.numpy()\n",
    "    num_instances = mask_array.shape[0]     \n",
    "    mask_array = np.moveaxis(mask_array, 0, -1)  \n",
    "    mask_array_instance = []\n",
    "    output = np.zeros_like(img)      \n",
    "    num,h,width = mask_array.shape\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        mask_array_instance.append(mask_array[:, :, i:(i+1)])\n",
    "        output = np.where(mask_array_instance[i] == True, 255, output)\n",
    "\n",
    "    filename = d[\"file_name\"]\n",
    "    start_index = filename.find(\"test/\")\n",
    "    substring = filename[start_index+5:]\n",
    "    out_path = os.path.join(data_path+'out_binary', substring)\n",
    "    #cv2.imwrite(out_path, output)\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dice score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true[y_true > 0] = 1\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true) + K.sum(y_pred) - intersection\n",
    "    dice_score = 2 * intersection / union\n",
    "    return dice_score.numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(v1, v2):\n",
    "    distance = 0\n",
    "    for i in range(len(v1)):\n",
    "        distance += (v1[i] - v2[i])**2\n",
    "    return distance / len(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif(len(ps) > len(list_of_lists)):\\n    ps = calculate_similarity_gt(ps, list_of_lists)\\nelse:\\n    list_of_lists = calculate_similarity_gt(list_of_lists, ps)\\n\\n\\n#print(ps)\\nprint(list_of_lists)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_similarity_gt(box1, box2):#ps比out_iou長 存回ps 若相反則import時倒轉\n",
    "    new_box = []\n",
    "    for index2, box2_item in enumerate(box2):\n",
    "        similarity = []\n",
    "        for index1, box1_item in enumerate(box1):\n",
    "            similarity.append(euclid(box2_item, box1_item))\n",
    "        new_box.append(box1[similarity.index(min(similarity))])\n",
    "    new_box = [tuple(b) for b in new_box]\n",
    "    return list(set(new_box))\n",
    "\n",
    "'''\n",
    "if(len(ps) > len(list_of_lists)):\n",
    "    ps = calculate_similarity_gt(ps, list_of_lists)\n",
    "else:\n",
    "    list_of_lists = calculate_similarity_gt(list_of_lists, ps)\n",
    "\n",
    "\n",
    "#print(ps)\n",
    "print(list_of_lists)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    # Calculate area of each bounding box\n",
    "    x1, y1, x2, y2 = box1\n",
    "    box1_area = abs((x2 - x1) * (y2 - y1))\n",
    "    x1, y1, x2, y2 = box2\n",
    "    box2_area = abs((x2 - x1) * (y2 - y1))\n",
    "    # Calculate intersection area\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    #if x1 < x2 and y1 < y2:\n",
    "    intersection_area = abs((x2 - x1) * (y2 - y1))\n",
    "    #else:\n",
    "    #intersection_area = np.abs(intersection_area)\n",
    "    #print(intersection_area)\n",
    "    # Calculate union area\n",
    "    \n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    #print(f\"box1{box1_area}\")\n",
    "    #print(f\"box2{box2_area}\")\n",
    "    #print(f\"union{union_area}\")\n",
    "    #print(f\"intertection{intersection_area}\")\n",
    "    if(union_area < 0):return 0\n",
    "\n",
    "    # Return IOU\n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n",
    "\n",
    "#for i, b1 in enumerate(out_iou):\n",
    "#    b2 = ps[i]\n",
    "#    iou = calculate_iou(b1, b2)\n",
    "#    print(f'IOU for boxes {i}: {iou}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU_file(filename):\n",
    "        json_file = filename\n",
    "        img = cv2.imread(json_file[:-5] + '.png')\n",
    "        outputs = predictor(img)#predicted outputs\n",
    "        with open(json_file, \"r\") as f:#get real json's coordinate\n",
    "            data = json.load(f)\n",
    "            annos = data[\"shapes\"]\n",
    "            ps = []\n",
    "            for anno in annos:\n",
    "                    px = [a[0] for a in anno['points']] # x coord\n",
    "                    py = [a[1] for a in anno['points']] # y-coord\n",
    "                    poly = [(x, y) for x, y in zip(px, py)] # poly for segmentation\n",
    "                    poly = [p for x in poly for p in x]\n",
    "                    ps.append(poly)\n",
    "        #print(ps)\n",
    "        ps = sorted(ps, key=lambda x: x[0])#get real json's coordinate\n",
    "        out_iou = outputs[\"instances\"].pred_boxes.tensor.cpu()#sort cuz it's in wrong order\n",
    "        #print(out_iou)\n",
    "        list_of_lists = [t.tolist() for t in out_iou]\n",
    "        list_of_lists = sorted(list_of_lists, key=lambda x:x[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        iou = 0\n",
    "        if(len(ps) < len(list_of_lists)):\n",
    "            list_of_lists = calculate_similarity_gt(list_of_lists, ps)\n",
    "            list_of_lists = sorted(list_of_lists, key=lambda x:x[0])\n",
    "            ps = sorted(ps, key=lambda x: x[0])\n",
    "            for i, b1 in enumerate(list_of_lists):\n",
    "                b2 = ps[i]\n",
    "                iou += calculate_iou(b1, b2)\n",
    "                #print(f'IOU for boxes {i}: {iou}')\n",
    "            iou = iou / len(list_of_lists)\n",
    "        elif(len(ps) > len(list_of_lists)):\n",
    "            ps = calculate_similarity_gt(ps, list_of_lists)\n",
    "            list_of_lists = sorted(list_of_lists, key=lambda x:x[0])\n",
    "            ps = sorted(ps, key=lambda x: x[0])\n",
    "            for i, b1 in enumerate(ps):\n",
    "                b2 = list_of_lists[i]\n",
    "                iou += calculate_iou(b1, b2)\n",
    "                #print(f'IOU for boxes {i}: {iou}')\n",
    "            iou = iou / len(ps)\n",
    "        else:\n",
    "            list_of_lists = sorted(list_of_lists, key=lambda x:x[0])\n",
    "            ps = sorted(ps, key=lambda x: x[0])\n",
    "            for i, b1 in enumerate(ps):\n",
    "                b2 = list_of_lists[i]\n",
    "                iou += calculate_iou(b2, b1)\n",
    "                #print(f'IOU for boxes {i}: {iou}')\n",
    "            iou = iou / len(ps)\n",
    "\n",
    "        if(iou > 1): iou = 1#有些bug>1我抓不到\n",
    "            \n",
    "        return iou\n",
    "        #print(list_of_lists)\n",
    "        #print(ps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 12:50:13.229 python[28698:4632506] +[CATransaction synchronize] called within transaction\n",
      "2023-01-09 12:50:13.281 python[28698:4632506] +[CATransaction synchronize] called within transaction\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 186\u001b[0m\n\u001b[1;32m    181\u001b[0m     window\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 186\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[30], line 135\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSegment\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    133\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()  \u001b[39m# 記錄目前的時間\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m--> 135\u001b[0m             values[\u001b[39m\"\u001b[39m\u001b[39m-FOLDER-\u001b[39m\u001b[39m\"\u001b[39m], values[\u001b[39m\"\u001b[39;49m\u001b[39m-FILE LIST-\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m     \u001b[39m# 使用 detectron2 偵測物件\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39m#filename = values[\"image\"]\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     label_path \u001b[39m=\u001b[39m filename\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import PySimpleGUI as sg\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import io\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "\n",
    "file_types = [(\"JPEG (*.jpg)\", \"*.jpg\"),\n",
    "              (\"All files (*.*)\", \"*.*\")]\n",
    "\n",
    "#config_file = r\"None\"\n",
    "#ckpt_file = r\"Please read your check point file\"\n",
    "#img_file = r\"Please read your img path\"\n",
    "#image_files = r\"Please read your img path\"\n",
    "\n",
    "# 建立 detectron2 的 predictor\n",
    "#cfg = model_zoo.get_config('COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml')\n",
    "#cfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n",
    "#cfg.MODEL.WEIGHTS = \"/Users/david/Desktop/DIP_final/outputModels1/model_final.pth\"\n",
    "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n",
    "\n",
    "#predictor = DefaultPredictor(cfg)\n",
    "\n",
    "file_types = [(\"JPEG (*.jpg)\", \"*.jpg\"),\n",
    "              (\"All files (*.*)\", \"*.*\")]\n",
    "file_list_column = [\n",
    "    [\n",
    "        sg.Listbox(\n",
    "            values=[], enable_events=True, size=(40, 30), key=\"-FILE LIST-\"\n",
    "        )\n",
    "    ],\n",
    "]\n",
    "height, width = 360, 360\n",
    "def main():\n",
    "    \n",
    "    layout = [\n",
    "        #sg.VSeperator(),\n",
    "\t\t#[sg.Text('config file',size=(16,1)), sg.In(config_file,size=(80,1), key='config'), sg.FileBrowse(file_types=file_types)],\n",
    "        #[sg.Text('model checkpoint',size=(16,1), auto_size_text=False), sg.In(ckpt_file,size=(80,1), key='ckpt'), sg.FileBrowse(file_types=file_types)],\n",
    "\t\t#[sg.Text('Path to image',size=(16,1)), sg.In(img_file,size=(80,1), key='image'), sg.FileBrowse(file_types=file_types),sg.Button(\"Load Image\"),],\n",
    "        \n",
    "        [\n",
    "        sg.Text(\"Image Folder\"),\n",
    "        sg.In(size=(80, 1), enable_events=True, key=\"-FOLDER-\"),\n",
    "        sg.FolderBrowse('瀏覽'),\n",
    "        ],\n",
    "\t\t[sg.Text('Device',size=(10,1)), sg.Combo(['cuda:0','cpu'],default_value='cpu',key='device')],\n",
    "        [sg.Text(size=(40, 1), key=\"-TOUT-\")],\n",
    "\t\t[sg.Text(\"File list\",size=(10,1)),sg.Text(\"Source Image\",size=(50,1)),sg.Text(\"Segmented Image\",size=(65,1))],\n",
    "        [sg.Column(file_list_column), sg.Image(key=\"source_image\",size=(height,width),background_color='white'),\n",
    "        sg.Image(key=\"segmented_image\",size=(height,width),background_color='white')],\n",
    "        [sg.Button('Segment'), sg.Exit()],\n",
    "        #[sg.Text('IOU',size=(4,1)),  sg.In(size=(100,1), key='iou')],\n",
    "        #[sg.Text('AP50',size=(4,1)),  sg.In(size=(100,1), key='ap')],\n",
    "        [sg.Text('Dice',size=(4,1)),  sg.In(size=(100,1), key='dice')],\n",
    "        [sg.Text('FPS',size=(4,1)),  sg.In(size=(100,1), key='fps')]\n",
    "        ]\n",
    "    window = sg.Window('detectron2', \n",
    "                   layout,\n",
    "                   default_element_size=(14,2),\n",
    "                   text_justification='right',\n",
    "                   auto_size_text=False\n",
    "                   # element_justification='c'\n",
    "                   )\n",
    "\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "\n",
    "        if event == \"Exit\" or event == sg.WIN_CLOSED:\n",
    "            break\n",
    "\n",
    "        if event == \"-FOLDER-\":\n",
    "            folder = values[\"-FOLDER-\"]\n",
    "            try:\n",
    "                file_list = os.listdir(folder)\n",
    "            except:\n",
    "                file_list = []\n",
    "\n",
    "            fnames = [\n",
    "                f\n",
    "                for f in file_list\n",
    "                if os.path.isfile(os.path.join(folder, f))\n",
    "                and f.lower().endswith((\".png\", \".gif\"))\n",
    "            ]\n",
    "            window[\"-FILE LIST-\"].update(fnames)\n",
    "        elif event == \"-FILE LIST-\":  \n",
    "            try:\n",
    "                filename = os.path.join(\n",
    "                    values[\"-FOLDER-\"], values[\"-FILE LIST-\"][0]\n",
    "                )\n",
    "                print(filename)\n",
    "                image = Image.open(filename)\n",
    "                image.thumbnail((400, 400))\n",
    "                bio = io.BytesIO()\n",
    "                # Actually store the image in memory in binary \n",
    "                image.save(bio, format=\"PNG\")\n",
    "                # Use that image data in order to \n",
    "                window[\"source_image\"].update(data=bio.getvalue())\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        if event == \"Load Image\":\n",
    "            start_time = time.time()  # 記錄目前的時間\n",
    "            filename = values[\"image\"]\n",
    "            image_name = os.path.basename(filename)\n",
    "            if os.path.exists(filename):\n",
    "                image = Image.open(values[\"image\"])\n",
    "                image.thumbnail((400, 400))\n",
    "                bio = io.BytesIO()\n",
    "                # Actually store the image in memory in binary \n",
    "                image.save(bio, format=\"PNG\")\n",
    "                # Use that image data in order to \n",
    "                window[\"source_image\"].update(data=bio.getvalue())\n",
    "\n",
    "        if event == \"image\":\n",
    "            file_path = values[\"image\"]\n",
    "            img = cv2.imread(file_path)\n",
    "            # 將圖片轉換成 PySimpleGUI 可以顯示的格式\n",
    "            img_bytes = cv2.imencode(\".png\", img)[1].tobytes()\n",
    "            # 將圖片顯示在 PySimpleGUI 的視窗中\n",
    "            window[\"source_image\"].update(data=img_bytes)\n",
    "\n",
    "            \n",
    "        if event == \"Segment\":\n",
    "            start_time = time.time()  # 記錄目前的時間\n",
    "            filename = os.path.join(\n",
    "                    values[\"-FOLDER-\"], values[\"-FILE LIST-\"][0]\n",
    "                )\n",
    "            # 使用 detectron2 偵測物件\n",
    "            #filename = values[\"image\"]\n",
    "            label_path = filename.replace(\"image\", \"label\")\n",
    "            mask_path = filename.replace(\"image\", \"mask\")\n",
    "            cfg = model_zoo.get_config('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml')\n",
    "            cfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n",
    "            cfg.MODEL.WEIGHTS = \"/Users/david/Desktop/DIP_final/segmentation/outputModels/model_final.pth\"\n",
    "            cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n",
    "            # 使用 detectron2 偵測圖片中的物體\n",
    "            img = cv2.imread(mask_path)\n",
    "            img = cv2.resize(img,(400,400))\n",
    "            #iou_num = IOU_file(label_path[:-3] + 'json')\n",
    "            print(label_path[:-3] + 'json')\n",
    "            outputs = predictor(img)\n",
    "            v = Visualizer(img[:, :, ::-1],\n",
    "                        metadata=microcontroller_metadata, \n",
    "                        scale=1, \n",
    "                        instance_mode=ColorMode.IMAGE_BW # removes the colors of unsegmented pixels\n",
    "            )\n",
    "            v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            mask_array = outputs['instances'].to(\"cpu\").pred_masks.numpy()\n",
    "            num_instances = mask_array.shape[0]     \n",
    "            mask_array = np.moveaxis(mask_array, 0, -1)  \n",
    "            mask_array_instance = []\n",
    "            output = np.zeros_like(img)   \n",
    "\n",
    "            for i in range(num_instances):\n",
    "                mask_array_instance.append(mask_array[:, :, i:(i+1)])\n",
    "                output = np.where(mask_array_instance[i] == True, 255, output)\n",
    "            # 將圖片轉換成 PySimpleGUI 可以顯示的格式\n",
    "            #print(output)\n",
    "            img = img.astype(np.float32)\n",
    "            output = output.astype(np.float32)\n",
    "            #print(dice_coef(img, output))\n",
    "            img_bytes = cv2.imencode(\".png\", output)[1].tobytes()\n",
    "            # 將圖片顯示在 PySimpleGUI 的視窗中\n",
    "            #window[\"iou\"].update(iou_num)\n",
    "            elapsed_time = time.time() - start_time  # 計算執行時間\n",
    "            window[\"dice\"].update(dice_coef(img, output))\n",
    "            window[\"fps\"].update(elapsed_time)\n",
    "            window[\"segmented_image\"].update(data=img_bytes)\n",
    "            \n",
    "\n",
    "            \n",
    "    window.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import cv2\n",
    "import numpy as np\n",
    "# 載入圖像\n",
    "image = cv2.imread(\"/Users/david/Desktop/class_data/Train/powder_uncover/mask/converted_ 0129.png\")\n",
    "\n",
    "# 定義紅色顏色範圍\n",
    "lower_red = np.array([0,0,100])\n",
    "upper_red = np.array([100,100,255])\n",
    "\n",
    "# 檢測圖像中的紅色\n",
    "mask = cv2.inRange(image, lower_red, upper_red)\n",
    "\n",
    "# 尋找輪廓\n",
    "contours, hierarchy  = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# 遍歷所有輪廓\n",
    "for contour in contours:\n",
    "    # 繪製輪廓\n",
    "    cv2.drawContours(image, contour, -1, (0,255,0), 3)\n",
    "objs = []\n",
    "points =[]\n",
    "px = []\n",
    "py = []\n",
    "for contour in contours:\n",
    "    # 遍歷輪廓中的所有點\n",
    "    for point in contour:\n",
    "        # 印出座標\n",
    "        px.append((point[0][0]))\n",
    "        py.append((point)[0][1])\n",
    "        points.append((point[0][0], point[0][1]))\n",
    "        poly = [point for x in points for point in x]\n",
    "    \n",
    "obj = {\n",
    "    \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "    #\"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "    \"segmentation\": [poly],\n",
    "    \"iscrowd\": 0\n",
    "}\n",
    "        \n",
    "objs.append(obj)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "import numpy as np\n",
    "with open(\"/Users/david/Desktop/DIP_final/segmentation/data/test/Converted_ 3153.json\") as f:\n",
    "    img_anns = json.load(f)\n",
    "    record = {}\n",
    "    annos = img_anns[\"shapes\"]\n",
    "    objs = []\n",
    "    print(annos)\n",
    "    for anno in annos:\n",
    "        px = [a[0] for a in anno['points']] # x coord\n",
    "        #print(px)\n",
    "        py = [a[1] for a in anno['points']] # y-coord\n",
    "        poly = [(x, y) for x, y in zip(px, py)] # poly for segmentation\n",
    "        poly = [p for x in poly for p in x]\n",
    "        obj = {\n",
    "\n",
    "            \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "            \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "            #\"segmentation\": [poly],\n",
    "            \"category_id\": classes.index(anno['label']),\n",
    "            \"iscrowd\": 0\n",
    "        }\n",
    "        objs.append(obj)\n",
    "    record[\"annotations\"] = objs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "with open(\"/Users/david/Desktop/malignant_238.json\") as f:\n",
    "    img_anns = json.load(f)\n",
    "    record = {}\n",
    "    annos = img_anns[\"shapes\"]\n",
    "    objs = []\n",
    "    for anno in annos:\n",
    "        print(1)\n",
    "        px = [a[0] for a in anno['points']] # x coord\n",
    "      \n",
    "        py = [a[1] for a in anno['points']] # y-coord\n",
    "        #print(py)\n",
    "        poly = [(x, y) for x, y in zip(px, py)] # poly for segmentation\n",
    "        poly = [p for x in poly for p in x]\n",
    "        #print(poly)\n",
    "        obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "        objs.append(obj)\n",
    "    record[\"annotations\"] = objs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true[y_true > 0] = 1\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true) + K.sum(y_pred) - intersection\n",
    "    dice_score = 2 * intersection / union\n",
    "    return dice_score.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691987"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "B = cv2.imread('/Users/david/Desktop/DIP_final/segmentation/data/out_binary/converted_ 0413.png')\n",
    "A = cv2.imread('/Users/david/Desktop/DIP_final/segmentation/data/test/converted_ 0413.png')\n",
    "# 将 A 和 B 转换为 float32 类型，方便计算\n",
    "A = A.astype(np.float32)\n",
    "B = B.astype(np.float32)\n",
    "dice_coef(A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "dir1 = '/Users/david/Desktop/DIP_final/segmentation/data/out/'\n",
    "dir2 = '/Users/david/Desktop/DIP_final/segmentation/data/test/'\n",
    "image1 = cv2.imread('/Users/david/Desktop/DIP_final/segmentation/data/out/converted_ 0425.png')\n",
    "image2 = cv2.imread('/Users/david/Desktop/DIP_final/segmentation/data/test/converted_ 0425.png')\n",
    "\n",
    "# display the image using matplotlib\n",
    "#plt.imshow(image)\n",
    "#plt.show()\n",
    "\n",
    "#for filename in [file for file in os.listdir(dir2) if file.endswith('.png')]:\n",
    "y_pred = cv2.imread(image1)/255.\n",
    "y_true = cv2.imread(image2)/255.\n",
    "#    y_pred = y_pred.reshape((-1, 480, 480, 3 ))\n",
    "#    y_true = y_true.reshape((-1, 480, 480, 3 ))\n",
    "#    dice_score = dice(dir2+filename, dir1+filename)\n",
    "#    print (\"Dice Coeff: {}\".format(dice_score))\n",
    "X= K.flatten(y_true)\n",
    "Y = K.flatten(y_pred)\n",
    "intersection = K.sum(X * Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv2.imread(dir1+filename)/255.\n",
    "y_true = cv2.imread(dir2+filename)/255.\n",
    "y_pred = y_pred.reshape((-1, 480, 480, 3 ))\n",
    "y_true = y_true.reshape((-1, 480, 480, 3 ))\n",
    "dice_score = dice_coefficient(dir2+filename, dir1+filename)\n",
    "print (\"Dice Coeff: {}\".format(dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_area = [0]\n",
    "for i in range(image1.shape[0]):\n",
    "    for j in range(image1.shape[1]):\n",
    "        if image1[i, j].all() == image2[i, j].all():\n",
    "            same_area[0] += 1\n",
    "print(same_area[0]/(480 * 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(image)\n",
    "for row in image:\n",
    "    for pixel in row:\n",
    "        if (pixel > 0).any():\n",
    "            print(pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv2.imread('/Users/david/Desktop/DIP_final/segmentation/data/out/Converted_ 0003.png')/255.\n",
    "y_true = cv2.imread('/Users/david/Desktop/DIP_final/segmentation/data/test/Converted_ 0003.png')/255.\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y_true.shape)\n",
    "\n",
    "y_pred = y_pred.reshape((-1, 480, 480, 3 ))\n",
    "y_true = y_true.reshape((-1, 480, 480, 3 ))\n",
    "\n",
    "#print(y_pred.shape)\n",
    "#print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score = dice_coefficient(y_true, y_pred)\n",
    "\n",
    "print (\"Dice Coeff: {}\".format(dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dicts(directory, classes):\n",
    "    dataset_dicts = []\n",
    "    for filename in [file for file in os.listdir(directory) if file.endswith('.png')]:\n",
    "        mask_file = os.path.join(directory, filename)\n",
    "\n",
    "        #要處理每場圖片的大小 要resize\n",
    "        resize_ratio = get_resize_ratio(mask_file)\n",
    "\n",
    "        with open(mask_file.replace(\".png\", \".json\")) as f:\n",
    "            img_anns = json.load(f)\n",
    "        # 載入圖像\n",
    "        image = cv2.imread(mask_file)\n",
    "\n",
    "        # 定義紅色顏色範圍\n",
    "        lower_black = np.array([0,0,0])\n",
    "        upper_black = np.array([0,0,0])\n",
    "\n",
    "        # 檢測圖像中黑色位置\n",
    "        black_mask = cv2.inRange(image, lower_black, upper_black)\n",
    "\n",
    "        # 將黑色位置設置為零，以達到檢測除了黑色以外的所有位置的目的\n",
    "        mask = cv2.bitwise_not(black_mask)\n",
    "\n",
    "        # 尋找輪廓\n",
    "        contours, hierarchy  = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "                \n",
    "        record = {}\n",
    "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"height\"] = 480\n",
    "        record[\"width\"] = 480\n",
    "      \n",
    "        annos = img_anns[\"shapes\"]\n",
    "        objs = []\n",
    "\n",
    "        for contour in contours:\n",
    "            # 繪製輪廓\n",
    "            cv2.drawContours(image, contour, -1, (0,255,0), 3)\n",
    "\n",
    "        \n",
    "        for contour in contours:\n",
    "            # 遍歷輪廓中的所有點\n",
    "            px = []\n",
    "            py = []\n",
    "            points = []\n",
    "            for point in contour:\n",
    "                # 印出座標\n",
    "                px.append((point[0][0]*resize_ratio))\n",
    "                py.append((point[0][1]*resize_ratio))\n",
    "                points.append((point[0][0]*resize_ratio, point[0][1]*resize_ratio))\n",
    "                poly = [point for x in points for point in x]\n",
    "        \n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": classes.index(annos[0]['label']),\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_dicts('/Users/david/Desktop/DIP_final/segmentation/data/a', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open(\"/Users/david/Desktop/malignant_238.json\") as f:\n",
    "    dataset_dicts = []\n",
    "    img_anns = json.load(f)\n",
    "    record = {}\n",
    "    annos = img_anns[\"shapes\"]\n",
    "    objs = []\n",
    "    for anno in annos:\n",
    "        print(1)\n",
    "        px = [a[0] for a in anno['points']] # x coord\n",
    "      \n",
    "        py = [a[1] for a in anno['points']] # y-coord\n",
    "        #print(py)\n",
    "        poly = [(x, y) for x, y in zip(px, py)] # poly for segmentation\n",
    "        poly = [p for x in poly for p in x]\n",
    "        #print(poly)\n",
    "        obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "        objs.append(obj)\n",
    "    record[\"annotations\"] = objs\n",
    "    dataset_dicts.append(record)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = cv2.imread('/Users/david/Desktop/DIP_final/segmentation/data/test/converted__ 0003.png')\n",
    "\n",
    "        # 定義紅色顏色範圍\n",
    "lower_black = np.array([0,0,0])\n",
    "upper_black = np.array([0,0,0])\n",
    "\n",
    "# 檢測圖像中黑色位置\n",
    "black_mask = cv2.inRange(image, lower_black, upper_black)\n",
    "\n",
    "# 將黑色位置設置為零，以達到檢測除了黑色以外的所有位置的目的\n",
    "mask = cv2.bitwise_not(black_mask)\n",
    "\n",
    "        # 尋找輪廓\n",
    "contours, hierarchy  = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\n",
    "for contour in contours:\n",
    "            # 繪製輪廓\n",
    "    cv2.drawContours(image, contour, -1, (0,255,0), 3)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "for contour in contours:\n",
    "        px = []\n",
    "        py = []\n",
    "        maxa =[]\n",
    "        maxy =[]\n",
    "        points = []\n",
    "        for point in contour:\n",
    "                px.append((point[0][0]*3))\n",
    "                py.append((point[0][1]*3))\n",
    "                points.append((point[0][0]*3, point[0][1]*3))\n",
    "                poly = [point for x in points for point in x]\n",
    "        print(\"another one\")   \n",
    "        print(f\"minx:x{min(px)}\")  \n",
    "        print(f\"minx:y{min(py)}\")      \n",
    "        maxa.append(max(px))\n",
    "        maxy.append(max(py))   \n",
    "        print(f\"fuck{px}\")\n",
    "        print(f\"py{py}\")\n",
    "        print(f\"maxx{maxa}\")\n",
    "        print(f\"maxy{maxy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PySimpleGUI as sg\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import io\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "\n",
    "file_types = [(\"JPEG (*.jpg)\", \"*.jpg\"),\n",
    "              (\"All files (*.*)\", \"*.*\")]\n",
    "\n",
    "#config_file = r\"None\"\n",
    "#ckpt_file = r\"Please read your check point file\"\n",
    "img_file = r\"Please read your img path\"\n",
    "image_files = r\"Please read your img path\"\n",
    "\n",
    "# 建立 detectron2 的 predictor\n",
    "#cfg = model_zoo.get_config('COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml')\n",
    "#cfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n",
    "#cfg.MODEL.WEIGHTS = \"/Users/david/Desktop/DIP_final/outputModels1/model_final.pth\"\n",
    "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n",
    "\n",
    "#predictor = DefaultPredictor(cfg)\n",
    "\n",
    "file_types = [(\"JPEG (*.jpg)\", \"*.jpg\"),\n",
    "              (\"All files (*.*)\", \"*.*\")]\n",
    "\n",
    "height, width = 360, 360\n",
    "def main():\n",
    "    \n",
    "    layout = [\n",
    "        #sg.VSeperator(),\n",
    "\t\t#[sg.Text('config file',size=(16,1)), sg.In(config_file,size=(80,1), key='config'), sg.FileBrowse(file_types=file_types)],\n",
    "        #[sg.Text('model checkpoint',size=(16,1), auto_size_text=False), sg.In(ckpt_file,size=(80,1), key='ckpt'), sg.FileBrowse(file_types=file_types)],\n",
    "\t\t[sg.Text('Path to image',size=(16,1)), sg.In(img_file,size=(80,1), key='image'), sg.FileBrowse(file_types=file_types),sg.Button(\"Load Image\"),],\n",
    "        \n",
    "        \n",
    "\t\t[sg.Text('Device',size=(16,1)), sg.Combo(['cuda:0','cpu'],default_value='cpu',key='device')],\n",
    "        [sg.Text(size=(40, 1), key=\"-TOUT-\")],\n",
    "\t\t[sg.Text(\"Source Image\",size=(16,1)),sg.Text(\"Segmented Image\",size=(65,1))],\n",
    "        [sg.Image(key=\"source_image\",size=(height,width),background_color='white'),\n",
    "        sg.Image(key=\"detected_image\",size=(height,width),background_color='white'),\n",
    "        sg.Image(key=\"segmented_image\",size=(height,width),background_color='white')],\n",
    "        [sg.Button('Segment'), sg.Exit()],\n",
    "        #[sg.Text('IOU',size=(3,1)),  sg.In(size=(100,1), key='iou')],\n",
    "        #[sg.Text('AP50',size=(3,1)),  sg.In(size=(100,1), key='ap')],\n",
    "        [sg.Text('Dice',size=(3,1)),  sg.In(size=(100,1), key='dice')],\n",
    "        [sg.Text('FPS',size=(3,1)),  sg.In(size=(100,1), key='fps')]\n",
    "        ]\n",
    "    window = sg.Window('detectron2', \n",
    "                   layout,\n",
    "                   default_element_size=(14,2),\n",
    "                   text_justification='right',\n",
    "                   auto_size_text=False\n",
    "                   # element_justification='c'\n",
    "                   )\n",
    "\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "\n",
    "\n",
    "\n",
    "        if event == \"Exit\" or event == sg.WIN_CLOSED:\n",
    "            break\n",
    "        if event == \"Load Image\":\n",
    "            filename = values[\"image\"]\n",
    "            image_name = os.path.basename(filename)\n",
    "            print(image_name)\n",
    "            if os.path.exists(filename):\n",
    "                image = Image.open(values[\"image\"])\n",
    "                image.thumbnail((400, 400))\n",
    "                bio = io.BytesIO()\n",
    "                # Actually store the image in memory in binary \n",
    "                image.save(bio, format=\"PNG\")\n",
    "                # Use that image data in order to \n",
    "                window[\"source_image\"].update(data=bio.getvalue())\n",
    "\n",
    "        if event == \"image\":\n",
    "            file_path = values[\"image\"]\n",
    "            img = cv2.imread(file_path)\n",
    "            # 將圖片轉換成 PySimpleGUI 可以顯示的格式\n",
    "            img_bytes = cv2.imencode(\".png\", img)[1].tobytes()\n",
    "            # 將圖片顯示在 PySimpleGUI 的視窗中\n",
    "            window[\"source_image\"].update(data=img_bytes)\n",
    "\n",
    "        '''\n",
    "        if event == \"Segment\":\n",
    "            # 使用 detectron2 偵測物件\n",
    "            filename = values[\"image\"]\n",
    "            label_path = filename.replace(\"image\", \"label\")\n",
    "\n",
    "            cfg = model_zoo.get_config('COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml')\n",
    "            cfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n",
    "            cfg.MODEL.WEIGHTS = \"/Users/david/Desktop/DIP_final/segmentation/data/out/model_final.pth\"\n",
    "            cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n",
    "            # 使用 detectron2 偵測圖片中的物體\n",
    "            img = cv2.imread(filename)\n",
    "            img = cv2.resize(img,(400,400))\n",
    "            iou_num = IOU_file(label_path[:-3] + 'json')\n",
    "            outputs = predictor(img)\n",
    "            v = Visualizer(img[:, :, ::-1],\n",
    "                        metadata=microcontroller_metadata, \n",
    "                        scale=1, \n",
    "                        instance_mode=ColorMode.IMAGE_BW # removes the colors of unsegmented pixels\n",
    "            )\n",
    "            v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            # 將圖片轉換成 PySimpleGUI 可以顯示的格式\n",
    "    \n",
    "            img_bytes = cv2.imencode(\".png\", v.get_image())[1].tobytes()\n",
    "            # 將圖片顯示在 PySimpleGUI 的視窗中\n",
    "            window[\"iou\"].update(iou_num)\n",
    "            window[\"ap\"].update(ap50['bbox']['AP50'])\n",
    "            window[\"detected_image\"].update(data=img_bytes)\n",
    "        '''\n",
    "        if event == \"Segment\":\n",
    "            # 使用 detectron2 偵測物件\n",
    "            filename = values[\"image\"]\n",
    "            label_path = filename.replace(\"image\", \"label\")\n",
    "            print(label_path)\n",
    "            mask_path = filename.replace(\"image\", \"mask\")\n",
    "            print(mask_path)\n",
    "            if event == \"Segment\":\n",
    "                cfg = model_zoo.get_config(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "                cfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n",
    "                cfg.MODEL.WEIGHTS = \"/Users/david/Desktop/DIP_final/segmentation/outputModels/model_final.pth\"\n",
    "                cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n",
    "            # 使用 detectron2 偵測圖片中的物體\n",
    "                img = cv2.imread(mask_path)\n",
    "                \n",
    "                img = cv2.resize(img,(400,400))\n",
    "                \n",
    "                iou_num = IOU_file(label_path[:-3] + 'json')\n",
    "                outputs = predictor(img)\n",
    "                \n",
    "                v = Visualizer(img[:, :, ::-1],\n",
    "                            metadata=microcontroller_metadata, \n",
    "                            scale=1, \n",
    "                            instance_mode=ColorMode.IMAGE_BW # removes the colors of unsegmented pixels\n",
    "                )\n",
    "                v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "                # 將圖片轉換成 PySimpleGUI 可以顯示的格式\n",
    "    \n",
    "                img_bytes = cv2.imencode(\".png\", v.get_image())[1].tobytes()\n",
    "                # 將圖片顯示在 PySimpleGUI 的視窗中\n",
    "                #window[\"iou\"].update(iou_num)\n",
    "                #window[\"ap\"].update(ap50['bbox']['AP50'])\n",
    "                window[\"segmented_image\"].update(data=img_bytes)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    window.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 08:57:44) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce243bcf4daa8f8238d5ab7a33151739a784881d93f47fa501704ba1f932b631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
